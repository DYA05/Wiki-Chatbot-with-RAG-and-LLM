{"cells":[{"cell_type":"markdown","metadata":{"id":"ruS4CYeBOCWW"},"source":["# RAG(Retrieval-Augmented Generation) based QA Chat bot"]},{"cell_type":"markdown","metadata":{"id":"9YmCCEhzOHF3"},"source":["## 0. 미션\n","참조\n","- 정보: https://cloud.google.com/vertex-ai/docs/generative-ai/open-models/use-gemma?hl=ko\n","- 2b: https://huggingface.co/google/gemma-2b\n","- 2b instruction tuning: https://huggingface.co/google/gemma-1.1-2b-it\n","- 7b: https://huggingface.co/google/gemma-7b\n","- 7b instruction tuning: https://huggingface.co/google/gemma-1.1-7b-it\n","- BM25: https://github.com/dorianbrown/rank_bm25\n","- SentenceTransformers: https://www.sbert.net/\n","\n","미션\n","- 질문에 대해서 적절한 문서를 검색하고, 검색된 문서에 근거해서 답변하는 RAG 챕봇을 만들어봅니다.\n","- 문서 검색은 2일차 실습 결과를 사용합니다.\n","- 검색된 문서에 근거해서 답변하는 기능은 구글의 gemma-2b-it SLLM을 사용합니다.\n","- 필요에 따라서 gemma-2b-it를 fine-tuning 합니다."]},{"cell_type":"markdown","metadata":{"id":"v0wmtPlzEs1Q"},"source":["## 1. 라이브러리 설치 (최초 한번만 실행)\n","- 라이브러리는 colab이 최초 실행 또는 종료 후 실행된 경우 한번만 실행하면 됩니다.\n","- GPU 메모리 부족등의 이유로 colab 세션을 다시 시작한 경우는 설치할 필요 없습니다.\n","- colab 세션을 다시 시작하려면 '런타임' >> '세션 다시 시작'을 선택하세요."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4dT1hV8-ER8a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1722132323328,"user_tz":-540,"elapsed":85895,"user":{"displayName":"유재하","userId":"18044117162550106446"}},"outputId":"f8a3e6c1-f529-4ee7-fb4b-911605e2d0df"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.7/130.7 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m89.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.9/170.9 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torch 2.3.1+cu121 requires nvidia-cublas-cu12==12.1.3.1; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n","torch 2.3.1+cu121 requires nvidia-cuda-cupti-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n","torch 2.3.1+cu121 requires nvidia-cuda-nvrtc-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n","torch 2.3.1+cu121 requires nvidia-cuda-runtime-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n","torch 2.3.1+cu121 requires nvidia-cudnn-cu12==8.9.2.26; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n","torch 2.3.1+cu121 requires nvidia-cufft-cu12==11.0.2.54; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n","torch 2.3.1+cu121 requires nvidia-curand-cu12==10.3.2.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n","torch 2.3.1+cu121 requires nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n","torch 2.3.1+cu121 requires nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n","torch 2.3.1+cu121 requires nvidia-nccl-cu12==2.20.5; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n","torch 2.3.1+cu121 requires nvidia-nvtx-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n","gcsfs 2024.6.1 requires fsspec==2024.6.1, but you have fsspec 2024.2.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.9/190.9 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.3/155.3 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.4/103.4 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.0/280.0 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.5/171.5 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.4/46.4 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m488.6/488.6 kB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install -q -U transformers==4.38.2\n","!pip install -q -U datasets==2.18.0\n","!pip install -q -U bitsandbytes==0.42.0\n","!pip install -q -U peft==0.9.0\n","!pip install -q -U trl==0.7.11\n","!pip install -q -U accelerate==0.27.2\n","!pip install -q -U rank_bm25==0.2.2\n","!pip install -q -U sentence-transformers==2.7.0\n","!pip install -q -U wikiextractor==3.0.6\n","!pip install -q -U konlpy==0.6.0"]},{"cell_type":"markdown","metadata":{"id":"uuNKlv3pFm9V"},"source":["## 2. 구글 드라이브 연결 (최초 한번만 실행)\n","- 구글 드라이브는 데이터 저장 및 학습 결과를 저장하기 위해서 사용합니다.\n","- 구글 드라이브는 colab이 최초 실행 또는 종료 후 실행된 경우 한번 만 연결하면 됩니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oxh3YfT1GLxh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714625666454,"user_tz":-540,"elapsed":2995,"user":{"displayName":"유재하","userId":"18044117162550106446"}},"outputId":"7ae9afa5-b4d8-4590-f635-ed3f33f3ff14"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"botOy_KdFhfP"},"source":["## *3. 환경 (매번 필수 실행)\n","- 환경은 colab 세션을 처음 시작하거나 다시 시작한 경우 실행되어야 합니다.\n","- 프로젝트 진행에 필요한 환경을 설정합니다."]},{"cell_type":"markdown","metadata":{"id":"cUaVEjBUHTeB"},"source":["### 3.1. 라이브러리 Import"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2pfHWwihFkne"},"outputs":[],"source":["import os\n","import glob\n","import json\n","\n","import numpy as np\n","import pandas as pd\n","from tqdm.auto import tqdm\n","\n","import torch\n","import konlpy\n","from rank_bm25 import BM25Okapi\n","from sentence_transformers import SentenceTransformer\n","from transformers import (AutoTokenizer,\n","                          AutoModelForCausalLM,\n","                          BitsAndBytesConfig,\n","                          pipeline)"]},{"cell_type":"markdown","metadata":{"id":"s7B4rxr4H8H1"},"source":["### 3.2. HuggingFace login\n","- 이번 프로젝트는 HuggingFace 로그인 해야만 진행이 가능합니다.\n","- HuggingFace 계정이 없다면 아래 URL에 접속해서 가입하시기 바랍니다.\n","  - https://huggingface.co/\n","- HuggingFace 로그인을 위해서 아래 URL에 접속해서 'User Access Token'을 생성하고 복사해서 Token에 입력하세요.\n","  - https://huggingface.co/settings/tokens"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QWo7Rb-KHR9a","colab":{"base_uri":"https://localhost:8080/","referenced_widgets":["81d32f1bf83c46b897382bdc6de7d6d6","591b6631bb384e11a35d85cce1cdd741","a737b9b35b094ab390f514f3f711cf71","1888212f177445d7b7c01d4dd99e8f3a","1b5d4e719e2942ba99c177974b8d12af","7ef1f9a8c4754e2e9020bf14b5523f26","2738d31dfac947f89772d49dc7c15d3a","568b93f361584628bc8d404e380d89c5","39f2335b0e3f41a6a031c0990b1e0237","7c4aa66d09a14780b2204caa82300b8c","fdf46c26dd9e4f41af5bab25476965e1","52b54930303e4fab9c8dde1d2d4d7ed8","3198253cdb7149d48f7a702170a7b7e1","e0ae0196609b41a084438a0a70bf97bb","fcf9dc6d4cfd42d5bfdecc939b4cb63a","b699c8057cbf4589a1f4ac90e81e0c85","1f2ed588dea34ac997a4f9af98c6825e"]},"executionInfo":{"status":"ok","timestamp":1714626126658,"user_tz":-540,"elapsed":12,"user":{"displayName":"유재하","userId":"18044117162550106446"}},"outputId":"aab867f7-da19-4804-9079-1873626f79c1"},"outputs":[{"output_type":"display_data","data":{"text/plain":["VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81d32f1bf83c46b897382bdc6de7d6d6"}},"metadata":{}}],"source":["from huggingface_hub import notebook_login\n","notebook_login()"]},{"cell_type":"code","source":["# access token을 복사하세요.\n","HF_TOKEN = \"\""],"metadata":{"id":"ZyiR76PQQPVY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m0RiT4MhJc0W"},"source":["### 3.3. 환경정보 설정\n","- WORKSPACE\n","  - 학습 데이터 및 학습결과를 저장하기 위한 경로입니다.\n","  - 필요할 경우 적당한 경로로 변경할 수 있습니다.\n","  - 경로를 변경 할 경우 전체 경로에 공백이 포함되지 않도록 주의해 주세요.\n","- SEARCH_MODEL_ID\n","  - 검색을 위한 SentenceTransformer 입니다.\n","  - 서울대학교 컴퓨터언어학_자연어처리 연구실에서 공개한 모델입니다.\n","  - https://huggingface.co/snunlp/KR-SBERT-V40K-klueNLI-augSTS\n","- SLLM_MODEL_ID\n","  - 문서에 근거해서 답변 기능을 위한 SLLM 입니다.\n","  - 구글에서 공개한 gemma-2b를 Instruction tunned한 버전입니다.\n","  - https://huggingface.co/google/gemma-2b-it\n","- CHUNK_FN\n","  - 문서를 일정한 단위로 분할해서 저장할 파일 이름\n","  - 데이터베이스를 대신하는 역할\n","  - 실제 기능을 만들 때는 DB를 사용해야 합니다.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_3oWzRGwJHBj"},"outputs":[],"source":["WORKSPACE = '/content/drive/MyDrive/nlp-project'\n","SEARCH_MODEL_ID = 'snunlp/KR-SBERT-V40K-klueNLI-augSTS'\n","SLLM_MODEL_ID = 'google/gemma-1.1-2b-it'\n","SLLM_MODEL_ID2 = 'google/gemma-1.1-7b-it'\n","CHUNK_FN = os.path.join(WORKSPACE, \"data\", \"chunk_db.json\")"]},{"cell_type":"markdown","source":["## 4. SLLM RAG tutorial (재시작 필요)\n","- SLLM에 질문과 근거 문서를 함께 입력하고 질문에 맞는 답변을 근거 문서로 부터 하도록 하는 과정을 이해하기 위한 과정입니다.\n","- 이 과정을 시작하기 전 colab 세션을 다시 시작하세요.\n","- colab 세션을 다시 시작해야 하는 이유는 LLM의 model의 크기가 너무 크기 때문에 GPU의 메모리를 초기화 하기 위해서 입니다."],"metadata":{"id":"dKtYwsbWVfXW"}},{"cell_type":"markdown","source":["### 4.1. model load with 4 bits\n","- 2B token을 가진 gemma를 그냥 로딩할 경우는 약 9G의 GPU vRAM이 필요합니다.\n","- 4bit 양자화를 할 경우 2.2G의 GPU vRAM 필요."],"metadata":{"id":"eTtY55mHSuMw"}},{"cell_type":"code","source":["# declare 4 bits quantize\n","quantization_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=torch.float16\n",")\n","# load 4 bits model\n","model = AutoModelForCausalLM.from_pretrained(SLLM_MODEL_ID,\n","                                             device_map='auto',\n","                                             quantization_config=quantization_config,\n","                                             token=HF_TOKEN)\n","# load tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(SLLM_MODEL_ID,\n","                                          add_special_tokens=True,\n","                                          token=HF_TOKEN)\n","tokenizer.padding_side = 'right'"],"metadata":{"id":"-parNbnOJ8rM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 4.2. pipeline\n","- https://huggingface.co/docs/transformers/main_classes/pipelines\n","- huggingface에서 inference를 쉽게 하기 위해 정의한 라이브러리."],"metadata":{"id":"vzrC3PRcV3zL"}},{"cell_type":"code","source":["pipe = pipeline(\"text-generation\",\n","                model=model,\n","                tokenizer=tokenizer,\n","                max_new_tokens=512)\n","pipe"],"metadata":{"id":"6v0OZYLUMOvP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 4.3. sllm prompt\n","- https://huggingface.co/google/gemma-1.1-2b-it\n","- 아래와 같은 형식이 gemma의 promt 형식 입니다.\n","```\n","<bos><start_of_turn>user\n","{content}<end_of_turn>\n","<start_of_turn>model\n","```\n","- NSMC 추론을 위한 프롬프트를 생성하는 과정입니다."],"metadata":{"id":"FOtCtWFbSsl6"}},{"cell_type":"code","source":["query = \"지미 카터 대통령이 졸업한 대학교는?\"\n","chunk_list = [\n","    \"지미 카터\\n제임스 얼 “지미” 카터 주니어(, 1924년 10월 1일~)는 민주당 출신 미국의 제39대 대통령 (1977-81)이다.\\n생애.\\n어린 시절.\\n지미 카터는 조지아주 섬터 카운티 플레인스 마을에서 태어났다.\\n조지아 공과대학교를 졸업하였다. 그 후 해군에 들어가 전함·원자력·잠수함의 승무원으로 일하였다. 1953년 미국 해군 대위로 예편하였고 이후 땅콩·면화 등을 가꿔 많은 돈을 벌었다. 그의 별명이 \\\"땅콩 농부\\\" (Peanut Farmer)로 알려졌다.\\n정계 입문.\\n1962년 조지아주 상원 의원 선거에서 낙선하였으나, 그 선거가 부정선거 였음을 입증하게 되어 당선되고, 1966년 조지아 주지사 선거에 낙선하지만, 1970년 조지아 주지사 선거에서 당선됐다. 대통령이 되기 전 조지아주 상원의원을 두번 연임했으며, 1971년부터 1975년까지 조지아 지사로 근무했다. 조지아 주지사로 지내면서, 미국에 사는 흑인 등용법을 내세웠다.\\n대통령 재임.\\n1976년 미합중국 제39대 대통령 선거에 민주당 후보로 출마하여 도덕주의 정책을 내세워서 많은 지지를 받았으며 제럴드 포드 대통령을 누르고 당선되었다.\\n카터 대통령은 에너지 개발을 촉구했으나 공화당의 반대로 무산되었다.\",\n","    \"수학\\n수학(, , math)은 수, 양, 구조, 공간, 변화 등의 개념을 다루는 학문이다. 널리 받아들여지는 명확한 정의는 없으나 현대 수학은 일반적으로 엄밀한 논리에 근거하여 추상적 대상을 탐구하며, 이는 규칙의 발견과 문제의 제시 및 해결의 과정으로 이루어진다. 수학은 그 발전 과정에 있어서 철학, 과학과 깊은 연관을 맺고 있으며, 엄밀한 논리와 특유의 추상성, 보편성에 의해 다른 학문들과 구별된다. 특히 수학은 과학의 여느 분야들과는 달리 자연계에서 관측되지 않는 개념들에 대해서까지 이론을 추상화시키는 특징을 보이는데, 수학자들은 그러한 개념들에 대한 추측을 제시하고 적절하게 선택된 정의와 공리로부터 엄밀한 연역을 거쳐 그 진위를 파악한다.\\n수학의 개념들은 기원전 600년 경에 활동하며 최초의 수학자로도 여겨지는 탈레스의 기록은 물론, 다른 고대 문명들에서도 찾아볼 수 있으며 인류의 문명과 함께 발전해 왔다. 오늘날 수학은 자연과학, 사회과학, 공학, 의학 등 거의 모든 학문에서도 핵심적인 역할을 하며 다양한 방식으로 응용된다.\\n수학을 의미하는 mathematics라는 단어는 '아는 모든 것', '배우는 모든 것'이라는 뜻의 고대 그리스어 'máthēma'(μάθημα) 및 그 활용형 mathēmatikós(μαθηματικός)에서 유래되었다.\",\n","    \"수학 상수\\n수학에서 상수란 그 값이 변하지 않는 불변량으로, 변수의 반대말이다. 물리 상수와는 달리, 수학 상수는 물리적 측정과는 상관없이 정의된다.\\n수학 상수는 대개 실수체나 복소수체의 원소이다. 우리가 이야기할 수 있는 상수는 (거의 대부분 계산 가능한) 정의가능한 수이다.\\n특정 수학 상수, 예를 들면 골롬-딕맨 상수, 프랑세즈-로빈슨 상수, formula_1, 레비 상수와 같은 상수는 다른 수학상수 또는 함수와 약한 상관관계 또는 강한 상관관계를 갖는다.\",\n","    \"문학\\n문학(文學, )은 언어를 예술적 표현의 제재로 삼아 새로운 의미를 창출하여, 인간과 사회를 진실되게 묘사하는 예술의 하위분야이다. 간단하게 설명하면, 언어를 통해 인간의 삶을 미적(美的)으로 형상화한 것이라고 볼 수 있다. 문학은 원래 문예(文藝)라고 부르는 것이 옳으며, 문학을 학문의 대상으로서 탐구하는 학문의 명칭 역시 문예학이다. 문예학은 음악사학, 미술사학 등과 함께 예술학의 핵심분야로서 인문학의 하위범주에 포함된다.\\n일반적으로 문학의 정의는 텍스트들의 집합이다. 각각의 국가들은 고유한 문학을 가질 수 있으며, 이는 기업이나 철학 조류, 어떤 특정한 역사적 시대도 마찬가지이다. 흔히 한 국가의 문학을 묶어서 분류한다. 예를 들어 고대 그리스어, 성서, 베오울프, 일리아드, 그리고 미국 헌법 등이 그러한 분류의 범주에 들어간다. 좀 더 일반적으로는 문학은 특정한 주제를 가진 이야기, 시, 희곡의 모음이라 할 수 있다. 이 경우, 이야기, 시, 그리고 희곡은 민족주의적인 색채를 띨 수도 아닐 수도 있다. 문학의 한 부분으로서 특정한 아이템을 구분 짓는 일은 매우 어려운 일이다. 어떤 사람들에게 \\\"문학\\\"은 어떠한 상징적인 기록의 형태로도 나타날 수 있는 것이다. (이를테면 이미지나 조각, 또는 문자로도 나타날 수 있다.) 그러나 또다른 사람들에게 있어 문학은 오직 문자로 이루어진 텍스트로 구성된 것만을 포함한다. 좀 더 보수적인 사람들은 그 개념이 꼭 물리적인 형태를 가진 텍스트여야 하고, 대개 그러한 형태는 종이 등의 눈에 보이는 매체에서 디지털 미디어까지 다양할 수 있다.\",\n","    \"화학\\n화학(化學)은 물질의 성질, 조성, 구조, 변화 및 그에 수반하는 에너지의 변화를 연구하는 자연과학(自然科學)의 한 분야이다. 물리학(物理學)도 역시 물질을 다루는 학문이지만, 물리학이 원소(元素)와 화합물(化合物)을 모두 포함한 물체의 운동과 에너지, 열적·전기적·광학적·기계적 속성을 다루고 이러한 현상으로부터 통일된 이론을 구축하려는 것과는 달리 화학에서는 물질 자체를 연구 대상으로 한다. 화학은 이미 존재하는 물질을 이용하여 특정한 목적에 맞는 새로운 물질을 합성하는 길을 제공하며, 이는 농작물(農作物)의 증산, 질병의 치료 및 예방, 에너지 효율 증대, 환경오염(環境汚染) 감소 등 여러 가지 이점을 제공한다.\\n어원.\\n화학은 연금술사들이 물질을 섞으며 발전시켰기 때문에 화학을 뜻하는 영어 ‘케미스트리(chemistry)’는 연금술을 뜻하는 단어 ‘알케미(alchemy)’에서 비롯하였다. 이는 다시 아랍어 ‘알 키미야(, al-kīmiyāʾ)’에서 왔는데, 이 단어의 어원에 대해서는 여러 가지 설이 있다.\\n‘화학(化學)’이란 단어는 물질의 변화를 다루는 학문이라는 점에 착안한 번역어이다. 이 번역어는 의 《항해술기(航海述奇)》(1866), 의 자연과학 교과서 《격물입문(格物入門)》(1866) 등에서 처음 쓰였다.\\n역사.\\n고대 화학(古代化學)\",\n","]"],"metadata":{"id":"xeUApEqfT2pN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["messages = [\n","    {\n","        \"role\": \"user\",\n","        \"content\": f\"\"\"당신이 가진 지식을 의존하지 말고 '문서1'부터 '문서5'를 참고해서 '질문'에 대해서 답변해 주세요.:\n","\n","문서5: {chunk_list[4]}\n","\n","문서4: {chunk_list[3]}\n","\n","문서3: {chunk_list[2]}\n","\n","문서2: {chunk_list[1]}\n","\n","문서1: {chunk_list[0]}\n","\n","질문: {query}\"\"\"\n","    }\n","]\n","prompt = pipe.tokenizer.apply_chat_template(messages,\n","                                            tokenize=False,\n","                                            add_generation_prompt=True)"],"metadata":{"id":"B8wHVUqYNKJn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(prompt)"],"metadata":{"id":"2c8yMyxdT7i7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 4.4. sllm inference\n","- 이전 단계에서 생성한 prompt를 이용해 추론하고 결과를 확인하는 과장입니다."],"metadata":{"id":"iAgMMfZGVMJs"}},{"cell_type":"code","source":["outputs = pipe(\n","    prompt,\n","    do_sample=True,\n","    temperature=0.2,\n","    top_k=50,\n","    top_p=0.95,\n","    add_special_tokens=True\n",")\n","outputs"],"metadata":{"id":"8arCVMAdT8w6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(outputs[0][\"generated_text\"])"],"metadata":{"id":"EXIDO8Q6Uwyg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(outputs[0][\"generated_text\"][len(prompt):])"],"metadata":{"id":"muhxBoJzU9L-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 4.5. sllm chatbot\n","- chatbot 형식의 QA 예 입니다."],"metadata":{"id":"t6jHfNM5Vkbd"}},{"cell_type":"code","source":["# 프롬프트 생성 함수\n","def gen_prompt(pipe, chunk_list, query):\n","    messages = [\n","        {\n","            \"role\": \"user\",\n","            \"content\": f\"\"\"당신이 가진 지식을 의존하지 말고 '문서1'부터 '문서5'를 참고해서 '질문'에 대해서 답변해 주세요.:\n","\n","문서5: {chunk_list[4]}\n","\n","문서4: {chunk_list[3]}\n","\n","문서3: {chunk_list[2]}\n","\n","문서2: {chunk_list[1]}\n","\n","문서1: {chunk_list[0]}\n","\n","질문: {query}\"\"\"\n","        }\n","    ]\n","    prompt = pipe.tokenizer.apply_chat_template(messages,\n","                                                tokenize=False,\n","                                                add_generation_prompt=True)\n","    return prompt"],"metadata":{"id":"Jbyo56u8U_oh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 프롬프트 생성 및 질문을 sllm에게 묻고 결과를 리턴하는 함수\n","def gen_response(pipe, chunk_list, query):\n","    prompt = gen_prompt(pipe, chunk_list, query)\n","\n","    outputs = pipe(\n","        prompt,\n","        do_sample=True,\n","        temperature=0.2,\n","        top_k=50,\n","        top_p=0.95,\n","        add_special_tokens=True\n","    )\n","    return outputs[0][\"generated_text\"][len(prompt):]"],"metadata":{"id":"KgpDQ7OXWNuY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["chunk_list = [\n","    \"지미 카터\\n제임스 얼 “지미” 카터 주니어(, 1924년 10월 1일~)는 민주당 출신 미국의 제39대 대통령 (1977-81)이다.\\n생애.\\n어린 시절.\\n지미 카터는 조지아주 섬터 카운티 플레인스 마을에서 태어났다.\\n조지아 공과대학교를 졸업하였다. 그 후 해군에 들어가 전함·원자력·잠수함의 승무원으로 일하였다. 1953년 미국 해군 대위로 예편하였고 이후 땅콩·면화 등을 가꿔 많은 돈을 벌었다. 그의 별명이 \\\"땅콩 농부\\\" (Peanut Farmer)로 알려졌다.\\n정계 입문.\\n1962년 조지아주 상원 의원 선거에서 낙선하였으나, 그 선거가 부정선거 였음을 입증하게 되어 당선되고, 1966년 조지아 주지사 선거에 낙선하지만, 1970년 조지아 주지사 선거에서 당선됐다. 대통령이 되기 전 조지아주 상원의원을 두번 연임했으며, 1971년부터 1975년까지 조지아 지사로 근무했다. 조지아 주지사로 지내면서, 미국에 사는 흑인 등용법을 내세웠다.\\n대통령 재임.\\n1976년 미합중국 제39대 대통령 선거에 민주당 후보로 출마하여 도덕주의 정책을 내세워서 많은 지지를 받았으며 제럴드 포드 대통령을 누르고 당선되었다.\\n카터 대통령은 에너지 개발을 촉구했으나 공화당의 반대로 무산되었다.\",\n","    \"수학\\n수학(, , math)은 수, 양, 구조, 공간, 변화 등의 개념을 다루는 학문이다. 널리 받아들여지는 명확한 정의는 없으나 현대 수학은 일반적으로 엄밀한 논리에 근거하여 추상적 대상을 탐구하며, 이는 규칙의 발견과 문제의 제시 및 해결의 과정으로 이루어진다. 수학은 그 발전 과정에 있어서 철학, 과학과 깊은 연관을 맺고 있으며, 엄밀한 논리와 특유의 추상성, 보편성에 의해 다른 학문들과 구별된다. 특히 수학은 과학의 여느 분야들과는 달리 자연계에서 관측되지 않는 개념들에 대해서까지 이론을 추상화시키는 특징을 보이는데, 수학자들은 그러한 개념들에 대한 추측을 제시하고 적절하게 선택된 정의와 공리로부터 엄밀한 연역을 거쳐 그 진위를 파악한다.\\n수학의 개념들은 기원전 600년 경에 활동하며 최초의 수학자로도 여겨지는 탈레스의 기록은 물론, 다른 고대 문명들에서도 찾아볼 수 있으며 인류의 문명과 함께 발전해 왔다. 오늘날 수학은 자연과학, 사회과학, 공학, 의학 등 거의 모든 학문에서도 핵심적인 역할을 하며 다양한 방식으로 응용된다.\\n수학을 의미하는 mathematics라는 단어는 '아는 모든 것', '배우는 모든 것'이라는 뜻의 고대 그리스어 'máthēma'(μάθημα) 및 그 활용형 mathēmatikós(μαθηματικός)에서 유래되었다.\",\n","    \"수학 상수\\n수학에서 상수란 그 값이 변하지 않는 불변량으로, 변수의 반대말이다. 물리 상수와는 달리, 수학 상수는 물리적 측정과는 상관없이 정의된다.\\n수학 상수는 대개 실수체나 복소수체의 원소이다. 우리가 이야기할 수 있는 상수는 (거의 대부분 계산 가능한) 정의가능한 수이다.\\n특정 수학 상수, 예를 들면 골롬-딕맨 상수, 프랑세즈-로빈슨 상수, formula_1, 레비 상수와 같은 상수는 다른 수학상수 또는 함수와 약한 상관관계 또는 강한 상관관계를 갖는다.\",\n","    \"문학\\n문학(文學, )은 언어를 예술적 표현의 제재로 삼아 새로운 의미를 창출하여, 인간과 사회를 진실되게 묘사하는 예술의 하위분야이다. 간단하게 설명하면, 언어를 통해 인간의 삶을 미적(美的)으로 형상화한 것이라고 볼 수 있다. 문학은 원래 문예(文藝)라고 부르는 것이 옳으며, 문학을 학문의 대상으로서 탐구하는 학문의 명칭 역시 문예학이다. 문예학은 음악사학, 미술사학 등과 함께 예술학의 핵심분야로서 인문학의 하위범주에 포함된다.\\n일반적으로 문학의 정의는 텍스트들의 집합이다. 각각의 국가들은 고유한 문학을 가질 수 있으며, 이는 기업이나 철학 조류, 어떤 특정한 역사적 시대도 마찬가지이다. 흔히 한 국가의 문학을 묶어서 분류한다. 예를 들어 고대 그리스어, 성서, 베오울프, 일리아드, 그리고 미국 헌법 등이 그러한 분류의 범주에 들어간다. 좀 더 일반적으로는 문학은 특정한 주제를 가진 이야기, 시, 희곡의 모음이라 할 수 있다. 이 경우, 이야기, 시, 그리고 희곡은 민족주의적인 색채를 띨 수도 아닐 수도 있다. 문학의 한 부분으로서 특정한 아이템을 구분 짓는 일은 매우 어려운 일이다. 어떤 사람들에게 \\\"문학\\\"은 어떠한 상징적인 기록의 형태로도 나타날 수 있는 것이다. (이를테면 이미지나 조각, 또는 문자로도 나타날 수 있다.) 그러나 또다른 사람들에게 있어 문학은 오직 문자로 이루어진 텍스트로 구성된 것만을 포함한다. 좀 더 보수적인 사람들은 그 개념이 꼭 물리적인 형태를 가진 텍스트여야 하고, 대개 그러한 형태는 종이 등의 눈에 보이는 매체에서 디지털 미디어까지 다양할 수 있다.\",\n","    \"화학\\n화학(化學)은 물질의 성질, 조성, 구조, 변화 및 그에 수반하는 에너지의 변화를 연구하는 자연과학(自然科學)의 한 분야이다. 물리학(物理學)도 역시 물질을 다루는 학문이지만, 물리학이 원소(元素)와 화합물(化合物)을 모두 포함한 물체의 운동과 에너지, 열적·전기적·광학적·기계적 속성을 다루고 이러한 현상으로부터 통일된 이론을 구축하려는 것과는 달리 화학에서는 물질 자체를 연구 대상으로 한다. 화학은 이미 존재하는 물질을 이용하여 특정한 목적에 맞는 새로운 물질을 합성하는 길을 제공하며, 이는 농작물(農作物)의 증산, 질병의 치료 및 예방, 에너지 효율 증대, 환경오염(環境汚染) 감소 등 여러 가지 이점을 제공한다.\\n어원.\\n화학은 연금술사들이 물질을 섞으며 발전시켰기 때문에 화학을 뜻하는 영어 ‘케미스트리(chemistry)’는 연금술을 뜻하는 단어 ‘알케미(alchemy)’에서 비롯하였다. 이는 다시 아랍어 ‘알 키미야(, al-kīmiyāʾ)’에서 왔는데, 이 단어의 어원에 대해서는 여러 가지 설이 있다.\\n‘화학(化學)’이란 단어는 물질의 변화를 다루는 학문이라는 점에 착안한 번역어이다. 이 번역어는 의 《항해술기(航海述奇)》(1866), 의 자연과학 교과서 《격물입문(格物入門)》(1866) 등에서 처음 쓰였다.\\n역사.\\n고대 화학(古代化學)\",\n","]"],"metadata":{"id":"DHEQmr5BscnP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["while True:\n","    query = input('질문 > ')\n","    query = query.strip()\n","    if len(query) == 0:\n","        break\n","    result = gen_response(pipe, chunk_list, query)\n","    print(f'답변 > {result}\\n\\n')"],"metadata":{"id":"Ey87FSqeWuur"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 5. SLLM no RAG 실습 (재시작 필요)\n","- RAG를 사용하지 않고 입력한 질문만을 이용해서 SLLM의 답변 능력을 확인하는 과정입니다.\n","- 이 과정을 시작하기 전 colab 세션을 다시 시작하세요.\n","- colab 세션을 다시 시작해야 하는 이유는 LLM의 model의 크기가 너무 크기 때문에 GPU의 메모리를 초기화 하기 위해서 입니다."],"metadata":{"id":"FDGQQ-IYQXD4"}},{"cell_type":"code","source":[],"metadata":{"id":"eKaf48fkRBWH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 5.1. SLLM no RAG\n","- 다음 순서로 동작하는 RAG chatbot을 구현하세요.\n","  - 사용자가 질문을 입력합니다.\n","  - 질문을 SLLM에 입력합니다.\n","  - 응답결과를 출력합니다."],"metadata":{"id":"cSCisSZoQp5o"}},{"cell_type":"markdown","source":["## 6. SLLM RAG with BM25 실습 (재시작 필요)\n","- BM25의 검색 기능과 SLLM의 추론 기능을 합쳐 RAG를 구성하는 실습입니다.\n","- BM25에서 tokenizer를 사용합니다.\n","- SLLM에서도 tokenizer를 사용합니다.\n","- 두 tokeinzer에 각각 다른 구분할 수 있는 이름을 변수 명으로 선언해야 합니다.\n","- 이 과정을 시작하기 전 colab 세션을 다시 시작하세요.\n","- colab 세션을 다시 시작해야 하는 이유는 LLM의 model의 크기가 너무 크기 때문에 GPU의 메모리를 초기화 하기 위해서 입니다."],"metadata":{"id":"Wfu2Qm8pvCvG"}},{"cell_type":"markdown","source":["### 6.1. bm25를 위한 전처리\n","- CHUNK_FN의 전체 문서를 사용하세요.\n","- BM25를 위해서 tokenizer를 정의하고 전처리를 실행합니다."],"metadata":{"id":"Tx_3igHi6xQ2"}},{"cell_type":"markdown","source":["#### 전처리된 파일 불러오기"],"metadata":{"id":"q8UFzERELe1f"}},{"cell_type":"code","source":["import gdown, os, glob, shutil\n","import csv\n","\n","def download_file(file_id, save_path) :\n","    if os.path.exists(save_path) :\n","        print(f'{save_path} 파일이 이미 존재합니다.')\n","        return\n","\n","    gdown.download(id=file_id, output=save_path, quiet=False)\n","\n","# 파일 다운로드\n","file_id = '1wrlIHXvZXLBoiPjH2nIPURdceMusQdrj'\n","download_file(file_id, 'chunk_list.csv')\n","file_id = '1fBk2ohMp1GJbGrSi8P0-zcIugW-csHUi'\n","download_file(file_id, \"tokenized_chunks.csv\")\n","\n","\n","#  문서를 이미 chunk 단위로 분할한 chunk_list 파일에서 불러오기\n","with open('/content/chunk_list.csv', \"r\", encoding=\"utf-8\") as f:\n","    reader = csv.reader(f)\n","    chunk_list = next(reader)\n","# chunk_list를 tokenize한 tokenized_chunks 불러오기\n","tokenized_chunks = []\n","with open('/content/tokenized_chunks.csv', 'r', encoding='utf-8') as csvfile:\n","    reader = csv.reader(csvfile)\n","    for row in reader:\n","        tokenized_chunks.append(row)"],"metadata":{"id":"hNq3hgKJLix8","executionInfo":{"status":"ok","timestamp":1714610211726,"user_tz":-540,"elapsed":114792,"user":{"displayName":"유재하","userId":"18044117162550106446"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c79350a1-405e-4e80-b233-14cac3c1b285"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading...\n","From (original): https://drive.google.com/uc?id=1wrlIHXvZXLBoiPjH2nIPURdceMusQdrj\n","From (redirected): https://drive.google.com/uc?id=1wrlIHXvZXLBoiPjH2nIPURdceMusQdrj&confirm=t&uuid=9a050aae-15b8-4e49-83cb-5715772081de\n","To: /content/chunk_list.csv\n","100%|██████████| 972M/972M [00:15<00:00, 60.8MB/s]\n","Downloading...\n","From (original): https://drive.google.com/uc?id=1fBk2ohMp1GJbGrSi8P0-zcIugW-csHUi\n","From (redirected): https://drive.google.com/uc?id=1fBk2ohMp1GJbGrSi8P0-zcIugW-csHUi&confirm=t&uuid=e29487c3-70ca-4241-94d9-5880a3c37b82\n","To: /content/tokenized_chunks.csv\n","100%|██████████| 848M/848M [00:13<00:00, 65.1MB/s]\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"uQ8xQI_1P3rT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### tokenized_chunks에 사용된 동일한 tokenizer 정의"],"metadata":{"id":"8KHQwiSgOLiM"}},{"cell_type":"code","source":["# 형태소 분석기를 이용한 tokeinizer 선언\n","# Komoran 품사표: https://docs.komoran.kr/firststep/postypes.html\n","KOMORAN = konlpy.tag.Komoran()\n","EXCLUDE = set(['JC', 'JKB', 'JKC', 'JKG', 'JKO', 'JKQ', 'JKS', 'JKV', 'JX',\n","               'EC', 'EF', 'EP', 'ETM', 'ETN'])\n","def tokenizer1(sent):\n","    tokens = []\n","    for w, t in KOMORAN.pos(sent):\n","        if t not in EXCLUDE:\n","            tokens.append(w)\n","    return tokens"],"metadata":{"id":"eEhH-_HGOTZP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"swqmKIoM6xQ3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 6.2. BM25 검색 함수\n","- bm25를 활용한 검색 기능을 함수로 구현하고 실험합니다."],"metadata":{"id":"HeJYWp7m6xQ3"}},{"cell_type":"code","source":["# bm25 api 생성\n","bm25 = BM25Okapi(tokenized_chunks)"],"metadata":{"id":"pbbch8TK6xQ4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","def query_bm25(bm25, query, tokens, top_n=10):\n","    doc_scores = bm25.get_scores(tokens)\n","    # score 순서로 정렬\n","    rank = np.argsort(-doc_scores)\n","    return rank[:top_n]"],"metadata":{"id":"SJTaXDw1QR2M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 검색어를 이용한 검색\n","query = \"수학교과서\"\n","tokenized_query = tokenizer1(query)\n","print(tokenized_query)\n","\n","doc_scores = bm25.get_scores(tokenized_query)\n","doc_scores"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E1fkGtvJwFnE","executionInfo":{"status":"ok","timestamp":1714622953493,"user_tz":-540,"elapsed":463,"user":{"displayName":"유재하","userId":"18044117162550106446"}},"outputId":"ac423b28-6266-48af-ce39-5d73f02e9d8a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['수학', '교과서']\n"]},{"output_type":"execute_result","data":{"text/plain":["array([0., 0., 0., ..., 0., 0., 0.])"]},"metadata":{},"execution_count":52}]},{"cell_type":"markdown","source":["### 6.3. SLLM RAG with BM25 구현\n","- 다음 순서로 동작하는 RAG chatbot을 구현하세요.\n","  - 사용자가 질문을 입력합니다.\n","  - 질문에 대해서 관련된 문서를 bm25를 이용해서 5개 구합니다.\n","  - bm25의 경우 응답이 5개 이하일 수 있습니다. 이 부분을 고려하세요.\n","  - 질문과 함께 문서 5개를 SLLM에 입력합니다.\n","  - 응답결과를 출력합니다."],"metadata":{"id":"FP6vn5y1yaEc"}},{"cell_type":"code","source":["# declare 4 bits quantize\n","quantization_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=torch.float16\n",")\n","# load 4 bits model\n","model = AutoModelForCausalLM.from_pretrained(SLLM_MODEL_ID,\n","                                             device_map='auto',\n","                                             quantization_config=quantization_config,\n","                                             token=HF_TOKEN)\n","# load tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(SLLM_MODEL_ID,\n","                                          add_special_tokens=True,\n","                                          token=HF_TOKEN)\n","tokenizer.padding_side = 'right'\n","\n","#pipe line\n","pipe = pipeline(\"text-generation\",\n","                model=model,\n","                tokenizer=tokenizer,\n","                max_new_tokens=512)\n","pipe"],"metadata":{"id":"shA882Cvzn2p","colab":{"base_uri":"https://localhost:8080/","height":68,"referenced_widgets":["e7cb4b12d6f74077a3b37b12b5f9c6d2","30a1122259ad45c4b727e52a8fbfe1d9","baf69db8f2cc4aefa82cdf842f73318c","cfba60f9cf9c4cc982272572a3e61433","7ccf8add4c3d4672a0e1061fe23758cf","7636c589cc9b4c3eb1fce18bc3cd3534","234893dfd5da4db989136ebc51f4f1cb","d71cee5477634239ba776cbf9c42eb27","6578ccfa5a1f44a09ea3924d2ab10b79","c03b94eb056d420c88fff40c5c171976","296753054e8d4c6a82f8d6150e3ebd11"]},"executionInfo":{"status":"ok","timestamp":1714622972899,"user_tz":-540,"elapsed":15308,"user":{"displayName":"유재하","userId":"18044117162550106446"}},"outputId":"4f61980f-937b-42ff-eb72-db0383a56922"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7cb4b12d6f74077a3b37b12b5f9c6d2"}},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["<transformers.pipelines.text_generation.TextGenerationPipeline at 0x7b3dd895b5b0>"]},"metadata":{},"execution_count":53}]},{"cell_type":"code","source":["def bm25_query(bm25, query, tokens, chunk_list):\n","\n","    result = query_bm25(bm25, query, tokens)\n","    print(result)\n","    info_list = [chunk_list[i] for i in result]\n","    print(info_list)\n","    messages = [\n","    {\n","        \"role\": \"user\",\n","        \"content\": f\"\"\"당신이 가진 지식을 의존하지 말고 '문서1'부터 '문서5'를 참고해서 '질문'에 대해서 답변해 주세요.:\n","\n","        문서5: {info_list[4]}\n","\n","        문서4: {info_list[3]}\n","\n","        문서3: {info_list[2]}\n","\n","        문서2: {info_list[1]}\n","\n","        문서1: {info_list[0]}\n","\n","        질문: {query}\"\"\"\n","    }\n","    ]\n","    prompt = pipe.tokenizer.apply_chat_template(messages,\n","                                            tokenize=False,\n","                                            add_generation_prompt=True)\n","    # print(prompt)\n","\n","    outputs = pipe(\n","        prompt,\n","        do_sample=True,\n","        temperature=0.2,\n","        top_k=50,\n","        top_p=0.95,\n","        add_special_tokens=True\n","    )\n","    print('답변 :', outputs[0][\"generated_text\"][len(prompt):])"],"metadata":{"id":"VHwSVr3SRVG9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["chunk_list[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":111},"id":"UZJec5D0bdyE","executionInfo":{"status":"ok","timestamp":1714622909126,"user_tz":-540,"elapsed":1375,"user":{"displayName":"유재하","userId":"18044117162550106446"}},"outputId":"f8fd2d05-8345-46ee-da93-b7b008a99c45"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'지미 카터\\n제임스 얼 “지미” 카터 주니어(, 1924년 10월 1일~)는 민주당 출신 미국의 제39대 대통령 (1977-81)이다.\\n생애.\\n어린 시절.\\n지미 카터는 조지아주 섬터 카운티 플레인스 마을에서 태어났다.\\n조지아 공과대학교를 졸업하였다. 그 후 해군에 들어가 전함·원자력·잠수함의 승무원으로 일하였다. 1953년 미국 해군 대위로 예편하였고 이후 땅콩·면화 등을 가꿔 많은 돈을 벌었다. 그의 별명이 \"땅콩 농부\" (Peanut Farmer)로 알려졌다.\\n정계 입문.\\n1962년 조지아주 상원 의원 선거에서 낙선하였으나, 그 선거가 부정선거 였음을 입증하게 되어 당선되고, 1966년 조지아 주지사 선거에 낙선하지만, 1970년 조지아 주지사 선거에서 당선됐다. 대통령이 되기 전 조지아주 상원의원을 두번 연임했으며, 1971년부터 1975년까지 조지아 지사로 근무했다. 조지아 주지사로 지내면서, 미국에 사는 흑인 등용법을 내세웠다.\\n대통령 재임.\\n1976년 미합중국 제39대 대통령 선거에 민주당 후보로 출마하여 도덕주의 정책을 내세워서 많은 지지를 받았으며 제럴드 포드 대통령을 누르고 당선되었다.\\n카터 대통령은 에너지 개발을 촉구했으나 공화당의 반대로 무산되었다.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":49}]},{"cell_type":"code","source":["query = '과학이 뭐야?'\n","tokenized_query = tokenizer1(query)\n","print(tokenized_query)\n","# score 계산\n","doc_scores = bm25.get_scores(tokenized_query)\n","print(doc_scores)\n","rank = np.argsort(-doc_scores)\n","print(rank)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IuRDXBgKZ6nq","executionInfo":{"status":"ok","timestamp":1714622977735,"user_tz":-540,"elapsed":1415,"user":{"displayName":"유재하","userId":"18044117162550106446"}},"outputId":"6903769c-c4e4-44b0-ed7e-3e0077e45efc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['과학', '뭐', '?']\n","[0. 0. 0. ... 0. 0. 0.]\n","[920583 726610 796938 ... 338991 338978 994855]\n"]}]},{"cell_type":"code","source":["query = '과학이 뭐야'\n","tokens = tokenizer1(query)\n","query_bm25(bm25, query, tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WabLm9pJY7wP","executionInfo":{"status":"ok","timestamp":1714622982389,"user_tz":-540,"elapsed":970,"user":{"displayName":"유재하","userId":"18044117162550106446"}},"outputId":"3152a885-dde1-416b-f2bd-d6e922e97602"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([726610,  64143, 796938, 869527, 749616, 843375, 727336, 829939,\n","       965064, 920583])"]},"metadata":{},"execution_count":56}]},{"cell_type":"code","source":["query = input()\n","tokens = tokenizer1(query)\n","bm25_query(bm25, query, tokens, chunk_list)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7XsoErDjT4Jr","executionInfo":{"status":"ok","timestamp":1714623017472,"user_tz":-540,"elapsed":10053,"user":{"displayName":"유재하","userId":"18044117162550106446"}},"outputId":"7f537087-ef9d-43d2-da5a-288d49f62111"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["과학이 뭐야?\n","[920583 726610 796938 869527 829939  63426 404952 919874 883756 681867]\n","['종말에 뭐하세요? 바쁘세요? 구해주실 수 있나요?\\n종말에 뭐하세요? 바쁘세요? 구해주실 수 있나요?(, WorldEnd)는 가레노 아키라 각본, 우에 작화의 일본의 라이트 노벨 시리즈이다. 이 시리즈의 제1권은 2014년 11월 1일 가도카와 스니커 문고를 통해 가도카와 쇼텐에 의해 출판되었다. 2016년 4월 제10권을 끝으로 시리즈는 마무리되었다. 후속작인 \"종말에 뭐하세요? 다시 한번 만날 수 있나요?\"(, SudaMoka)의 경우 2016년 4월 제1권부터 출판을 시작했다.', '놀면 뭐하니?\\n《놀면 뭐하니?》는 매주 토요일 오후 6시 25분에 방영 중인 텔레비전 프로그램이다.', '여우 같은 게 뭐가 나빠?\\n《여우 같은 게 뭐가 나빠?》()는 2020년 10월부터 TV 아사히 계열에서 방송 예정인 버라이어티 쇼이다.', 'TV야 뭐하니?\\nTV야 뭐하니?는 2006년에 대교어린이TV에서 방송한 프로그램이다.\\n진행자.\\n이선우, 임준수\\n내용.\\n알루 따라잡기\\nTV 소식통', '얼짱시대 요즘뭐해?\\n《얼짱시대 요즘뭐해?》는 2020년부터 2021년까지 유튜브에서 방송된 웹 예능이다. 역대 얼짱시대의 얼짱들을 다시 만나 근황을 확인하는 프로그램이다.', '아바르어\\n어떻게 느꼈어요?\\nDur tsar chib?\\n이름은 뭐에요?\\nChan son due bugeb?\\n몇살이에요?\\nKiwe mun inev vigev?\\n어디에 가세요?', '멘토\\n멘토가 뭐에요?', '14대 닥터\\n성격.\\n14대 닥터는 10대 닥터와 거의 비슷한 것으로 보인다. 억양은 10대와 똑같이 에스추어리 영어를 쓰며, 자신의 모습이 새 모습이 아닌 이전에 겪었던 모습임을 알게 되자, 10대 닥터가 종종 외치던 \"뭐야? 뭐야? 뭐야?\" (What? What? What?)를 그대로 따라하는 모습을 보였다. 여기에 10대 닥터가 재생성한 직후 \"치아가 새 것\"이라는 소감을 그대로 따라, \"익숙한 치아다\"라는 대사도 날린다. 이는 14대 닥터가 10대 닥터와 동일한 인격이 아닌 이상, 그 모습과 성격이 굉장히 유사하게 설정되었다는 의미이기도 하다.\\n모습.\\n14대 닥터는 10대 닥터와 똑같은 모습에 똑같은 목소리, 똑같은 헤어스타일을 지녔으나 살짝 나이가 든 모습이다. 의상 역시 매우 비슷하나 갈색 외투에 푸른색 오버코트를 입어, 10대 닥터가 입던 갈색 트렌치 코트와는 조금 다른 모습이다. 14대 닥터의 의상은 보통 재생성 후 진정되고 나서 직접 자기가 입을 옷을 고르던 이전의 닥터들과는 달리, 13대 닥터가 재생성하는 과정에서 함께 새롭게 만들어져 나타난 것도 주목할 만한 부분이다.', \"새집이라고 했는데 이 얼룩은 뭐죠\\n새집이라고 했는데 이 얼룩은 뭐죠(Heard It's A New House, What's This Stain?)는 한국에서 제작된 남현미 감독의 2002년 영화이다. 김태언 등이 주연으로 출연하였다.\", '전재민\\n전재민은 대한민국의 희극인 겸 가수이며, 소속그룹인 코쿤으로 활동하고 있다.\\n뮤직 비디오.\\n뭐라고? (What?) - 코쿤 (KOKOON)']\n","답변 : 문서에서는 과학에 대한 내용이 없습니다. 따라서 질문에 답변할 수 없습니다.\n"]}]},{"cell_type":"markdown","source":["## 7. SLLM RAG with SentenceTransformers 실습 (재시작 필요)\n","- SentenceTransformers의 검색 기능과 SLLM의 추론 기능을 합쳐 RAG를 구성하는 실습입니다.\n","- SentenceTransformers도 model을 사용합니다.\n","- SLLM도 model을 사용합니다.\n","- 두 model에 각각 다른 구분할 수 있는 이름을 변수 명으로 선언해야 합니다.\n","- 이 과정을 시작하기 전 colab 세션을 다시 시작하세요.\n","- colab 세션을 다시 시작해야 하는 이유는 LLM의 model의 크기가 너무 크기 때문에 GPU의 메모리를 초기화 하기 위해서 입니다."],"metadata":{"id":"gy4hDqsl8lTA"}},{"cell_type":"markdown","source":["### 7.1. SentenceTransformers를 위한 전처리\n","- CHUNK_FN의 전체 문서를 사용하세요."],"metadata":{"id":"uHcQJeJf8UoQ"}},{"cell_type":"code","source":["import gdown, os, glob, shutil\n","import csv\n","\n","def download_file(file_id, save_path) :\n","    if os.path.exists(save_path) :\n","        print(f'{save_path} 파일이 이미 존재합니다.')\n","        return\n","\n","    gdown.download(id=file_id, output=save_path, quiet=False)\n","\n","# 파일 다운로드\n","file_id = '1wrlIHXvZXLBoiPjH2nIPURdceMusQdrj'\n","download_file(file_id, 'chunk_list.csv')\n","\n","#  문서를 이미 chunk 단위로 분할한 chunk_list 파일에서 불러오기\n","with open('/content/chunk_list.csv', \"r\", encoding=\"utf-8\") as f:\n","    reader = csv.reader(f)\n","    chunk_list = next(reader)\n"],"metadata":{"id":"SGqHB-Vl8UoQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# SentenceTransformer 모델 생성\n","model_st = SentenceTransformer(SEARCH_MODEL_ID)"],"metadata":{"id":"a1aIpjAyU5qE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# chunk embeddings 생성\n","chunk_embeddings = model_st.encode(chunk_list)\n","chunk_embeddings.shape"],"metadata":{"id":"9yoBTu_uU9PT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714621844563,"user_tz":-540,"elapsed":7939898,"user":{"displayName":"유재하","userId":"18044117162550106446"}},"outputId":"4bd93ed5-48fe-4c3c-d9d3-46b50b484f29"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(994856, 768)"]},"metadata":{},"execution_count":33}]},{"cell_type":"code","source":["type(chunk_embeddings)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xq6H64k5HAWx","executionInfo":{"status":"ok","timestamp":1714621847373,"user_tz":-540,"elapsed":6,"user":{"displayName":"유재하","userId":"18044117162550106446"}},"outputId":"c8cd75ef-ae0a-4711-d63f-23c3c1e5d834"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["numpy.ndarray"]},"metadata":{},"execution_count":34}]},{"cell_type":"code","source":["chunk_embeddings[:2]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sTCCTHGOd11d","executionInfo":{"status":"ok","timestamp":1714622530826,"user_tz":-540,"elapsed":4,"user":{"displayName":"유재하","userId":"18044117162550106446"}},"outputId":"4f561e02-150e-4e40-a6a6-afbd97c564bf"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[-1.1006908 , -0.37803537, -0.39443707, ...,  0.14159042,\n","        -0.37158853, -0.21305898],\n","       [-0.49802774,  0.06850217, -0.2888093 , ..., -0.09489366,\n","         0.12228046, -0.23319054]], dtype=float32)"]},"metadata":{},"execution_count":36}]},{"cell_type":"code","source":["# 배열 저장\n","np.save('/content/drive/MyDrive/nlp-project/chunk_embeddings.npy', chunk_embeddings)"],"metadata":{"id":"psb44EemHQJW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import gdown, os, glob, shutil\n","import numpy as np\n","def download_file(file_id, save_path) :\n","    if os.path.exists(save_path) :\n","        print(f'{save_path} 파일이 이미 존재합니다.')\n","        return\n","\n","    gdown.download(id=file_id, output=save_path, quiet=False)\n","\n","# 파일 다운로드\n","file_id = '1212HCwiIB2aVBr-e4afjaXcv56npdUhw'\n","download_file(file_id, 'chunk_embeddings.npy')\n","\n","chunk_embeddings = np.load('/content/chunk_embeddings.npy')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o_5sbEm8d-Ug","executionInfo":{"status":"ok","timestamp":1714624004000,"user_tz":-540,"elapsed":23479,"user":{"displayName":"유재하","userId":"18044117162550106446"}},"outputId":"2f3bee13-fba4-4a79-9414-5113d29daf09"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading...\n","From (original): https://drive.google.com/uc?id=1212HCwiIB2aVBr-e4afjaXcv56npdUhw\n","From (redirected): https://drive.google.com/uc?id=1212HCwiIB2aVBr-e4afjaXcv56npdUhw&confirm=t&uuid=31414ad8-194b-4069-815e-6bed53c8d7ed\n","To: /content/chunk_embeddings.npy\n","100%|██████████| 3.06G/3.06G [00:15<00:00, 192MB/s]\n"]}]},{"cell_type":"markdown","source":["### 7.2. SentenceTransformers 검색 함수\n","- SentenceTransformers를 활용한 검색 기능을 함수로 구현하고 실험합니다."],"metadata":{"id":"BgieRr5W8UoR"}},{"cell_type":"code","source":["def query_sentence_transformer(model, chunk_embeddings, query, top_n=10):\n","    query_embedding = model_st.encode([query])\n","    # score 계산\n","    doc_scores = np.matmul(chunk_embeddings, query_embedding.T)\n","    doc_scores = doc_scores.reshape(-1)\n","    # score 순서로 정렬\n","    rank = np.argsort(-doc_scores)\n","    # top-n\n","    result = []\n","    return rank[:top_n]"],"metadata":{"id":"yg91TXgL8UoR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["query = '과학에 대하여 알려줘'\n","query_sentence_transformer(model_st, chunk_embeddings, query)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_1k6f46_A2Tg","executionInfo":{"status":"ok","timestamp":1714623253066,"user_tz":-540,"elapsed":1011,"user":{"displayName":"유재하","userId":"18044117162550106446"}},"outputId":"4abfcdf3-8daa-4e5d-d352-ea115c3d80e6"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([466576, 584236, 555455, 941465, 493317, 745323, 748368,  43467,\n","       166789, 175165])"]},"metadata":{},"execution_count":67}]},{"cell_type":"markdown","source":["### 7.3. SLLM RAG with SentenceTransformers를 구현\n","- 다음 순서로 동작하는 RAG chatbot을 구현하세요.\n","  - 사용자가 질문을 입력합니다.\n","  - 질문에 대해서 관련된 문서를 SentenceTransformers를를 이용해서 5개 구합니다.\n","  - 질문과 함께 문서 5개를 SLLM에 입력합니다.\n","  - 응답결과를 출력합니다."],"metadata":{"id":"RNV_4xGy8lTG"}},{"cell_type":"code","source":["def SentenceTransformers_query(model, chunk_embeddings, query, chunk_list):\n","\n","    result = query_sentence_transformer(model, chunk_embeddings, query)\n","    print(result)\n","    info_list = [chunk_list[i] for i in result]\n","    print(info_list)\n","    messages = [\n","    {\n","        \"role\": \"user\",\n","        \"content\": f\"\"\"당신이 가진 지식을 의존하지 말고 '문서1'부터 '문서5'를 참고해서 '질문'에 대해서 답변해라.:\n","\n","        문서5: {info_list[4]}\n","\n","        문서4: {info_list[3]}\n","\n","        문서3: {info_list[2]}\n","\n","        문서2: {info_list[1]}\n","\n","        문서1: {info_list[0]}\n","\n","        질문: {query}\"\"\"\n","    }\n","    ]\n","    prompt = pipe.tokenizer.apply_chat_template(messages,\n","                                            tokenize=False,\n","                                            add_generation_prompt=True)\n","    # print(prompt)\n","\n","    outputs = pipe(\n","        prompt,\n","        do_sample=True,\n","        temperature=0.2,\n","        top_k=50,\n","        top_p=0.95,\n","        add_special_tokens=True\n","    )\n","    print('답변 \\n', outputs[0][\"generated_text\"][len(prompt):])"],"metadata":{"id":"xY4wy5Cv8lTH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["query = '과학이 뭐야?'\n","SentenceTransformers_query(model_st, chunk_embeddings, query, chunk_list)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a8H8epOkAxMj","executionInfo":{"status":"ok","timestamp":1714623295357,"user_tz":-540,"elapsed":3682,"user":{"displayName":"유재하","userId":"18044117162550106446"}},"outputId":"7319be9b-61e0-431b-d0e9-4d676a7374e4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[466576 584236 555455 513998 748368 175165 745323   6554 341161  28176]\n","['과학로\\n과학로는 다음을 가리키는 말이다.', '과학원\\n과학원의 다른 뜻은 다음과 같다.', '과학 산업 박물관\\n과학 산업 박물관은 다음과 같은 박물관이 있다.', '부트스트랩 모형\\n부트스트랩 모형()은 양자 색역학 이전에 통용되던 물리 모형으로, 일반론으로부터 복잡한 이론을 완성하고자 하는 연구 분야이다.', '운동 법칙\\n물리학에서 여러 운동 법칙이 이론이 개발되었다. 그 중 다음을 포함한다:', '우주과학\\n우주과학()은 우주에 대해 연구하는 과학을 한데 묶어 부르는 이름이다.\\n연구 현상.\\n이외에도 우주에서 일어나는 여러 현상들을 연구한다.', '이공주\\n이공주는 다음 사람을 가리킨다.', '다윈\\n다윈의 다른 뜻은 다음과 같다.', '리가 1\\n리가 1의 다른 뜻은 다음과 같다.', '이론물리학\\n1900년대 중반을 지나는 때의 이론 물리 경향은 이전까지 물리학에서 쓰지 않던 수학들이 현대 물리의 전반에 응용이 되어 널리 쓰이게 되었다는 점이다. 리 군과 리 대수는 게이지 대칭성에 쓰이며, 일반 상대성 이론, 양자장론에서 장을 기술하는데 준 리만 다양체를 사용한다. 그러나 초끈 이론은 2021년까지 검증된것이 하나도 없어서 많은 비판을 받고 있으며, 물리학이라기보다는 아직은 양자장론을 도와주는 방법론 정도에 그치고 있다. 이론 물리학이라기보다는 수리 물리학에 가깝다고 해야겠다. 현대 물리학에서 실험과 거리가 있으면서 지나치게 수학에 의존이 심한 이론들이 나오는 경향은 물리학자들의 경계를 받기도 하며, 특히 과학을 하는데 수학적 아름다움 비슷한걸 고려하는 것 같은 비과학적인 방식을 지나치게 중요시하는 물리학자들이 비판을 받기도 한다.\\n대표적인 이론물리학자로는 알베르트 아인슈타인, 막스 플랑크, 에르빈 슈뢰딩거, 하이젠베르크, 리처드 파인만, 로버트 오펜하이머, 스티븐 와인버그, 폴 디랙, 스티븐 호킹,킵 손 등이 있다.']\n","답변 \n"," **질문: 과학이 뭐야?**\n","\n","문서1에서 과학은 물리학과 같은 과학으로, 물리적 현상을 이해하고 예측하는 것을 의미한다는 것을 알 수 있습니다.\n"]}]},{"cell_type":"markdown","source":["## 8. RAG 성능 확장 실습 (재시작 필요)\n","- CHUNK_FN의 전체 문서를 원하는 데이터로 구성해 보세요.\n","  - 예) 다른 위키 문서들\n","  - 예) 뉴스\n","  - 예) 책 또는 메뉴얼\n","- 프롬프트를 개선해 보세요.\n","- 'google/gemma-1.1-7b-it' 까지는 T4에서 겨우 돌아갑니다. 큰 LLM을 적용해 보세요.\n","- 다양한 방법으로 자신만의 RAG QA봇을 만들어 보세요.\n","- 이 과정을 시작하기 전 colab 세션을 다시 시작하세요.\n","- colab 세션을 다시 시작해야 하는 이유는 LLM의 model의 크기가 너무 크기 때문에 GPU의 메모리를 초기화 하기 위해서 입니다.\n"],"metadata":{"id":"UqzqXHO8JgQk"}},{"cell_type":"markdown","source":["- 입력하려는 문서의 수를 늘리니 메모리 부족문제가 발생하여 2B 사용하여\n","- bm25와 SentenceTransformers를 통해 얻은 유사한 정보를 각 10개씩 입력하여 RAG 실행"],"metadata":{"id":"FqANt_FXln6O"}},{"cell_type":"code","source":["import gdown, os, glob, shutil\n","import csv\n","\n","def download_file(file_id, save_path) :\n","    if os.path.exists(save_path) :\n","        print(f'{save_path} 파일이 이미 존재합니다.')\n","        return\n","\n","    gdown.download(id=file_id, output=save_path, quiet=False)\n","\n","# 파일 다운로드\n","file_id = '1wrlIHXvZXLBoiPjH2nIPURdceMusQdrj'\n","download_file(file_id, 'chunk_list.csv')\n","file_id = '1fBk2ohMp1GJbGrSi8P0-zcIugW-csHUi'\n","download_file(file_id, \"tokenized_chunks.csv\")\n","\n","\n","#  문서를 이미 chunk 단위로 분할한 chunk_list 파일에서 불러오기\n","with open('/content/chunk_list.csv', \"r\", encoding=\"utf-8\") as f:\n","    reader = csv.reader(f)\n","    chunk_list = next(reader)\n","# chunk_list를 tokenize한 tokenized_chunks 불러오기\n","tokenized_chunks = []\n","with open('/content/tokenized_chunks.csv', 'r', encoding='utf-8') as csvfile:\n","    reader = csv.reader(csvfile)\n","    for row in reader:\n","        tokenized_chunks.append(row)\n","# chunk_embeddings\n","file_id = '1212HCwiIB2aVBr-e4afjaXcv56npdUhw'\n","download_file(file_id, 'chunk_embeddings.npy')\n","\n","chunk_embeddings = np.load('/content/chunk_embeddings.npy')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yLamq5wmm33Z","executionInfo":{"status":"ok","timestamp":1714626169026,"user_tz":-540,"elapsed":41471,"user":{"displayName":"유재하","userId":"18044117162550106446"}},"outputId":"19cd1ab5-7b1f-4d89-f96a-1702cc10e7b4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["chunk_list.csv 파일이 이미 존재합니다.\n","tokenized_chunks.csv 파일이 이미 존재합니다.\n","chunk_embeddings.npy 파일이 이미 존재합니다.\n"]}]},{"cell_type":"code","source":["# 형태소 분석기를 이용한 tokeinizer 선언\n","# Komoran 품사표: https://docs.komoran.kr/firststep/postypes.html\n","KOMORAN = konlpy.tag.Komoran()\n","EXCLUDE = set(['JC', 'JKB', 'JKC', 'JKG', 'JKO', 'JKQ', 'JKS', 'JKV', 'JX',\n","               'EC', 'EF', 'EP', 'ETM', 'ETN'])\n","def tokenizer1(sent):\n","    tokens = []\n","    for w, t in KOMORAN.pos(sent):\n","        if t not in EXCLUDE:\n","            tokens.append(w)\n","    return tokens"],"metadata":{"id":"ruUmR-Q5rE4-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# bm25 api 생성\n","bm25 = BM25Okapi(tokenized_chunks)\n","\n","def query_bm25(bm25, query, tokens, top_n=10):\n","    doc_scores = bm25.get_scores(tokens)\n","    # score 순서로 정렬\n","    rank = np.argsort(-doc_scores)\n","    return rank[:top_n]"],"metadata":{"id":"yJaI8ppXm6cL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def query_sentence_transformer(model, chunk_embeddings, query, top_n=10):\n","    query_embedding = model_st.encode([query])\n","    # score 계산\n","    doc_scores = np.matmul(chunk_embeddings, query_embedding.T)\n","    doc_scores = doc_scores.reshape(-1)\n","    # score 순서로 정렬\n","    rank = np.argsort(-doc_scores)\n","    # top-n\n","    result = []\n","    return rank[:top_n]"],"metadata":{"id":"m9l00AZZqVsx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# SentenceTransformer 모델 생성\n","model_st = SentenceTransformer(SEARCH_MODEL_ID)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jAIxLIcQng4o","executionInfo":{"status":"ok","timestamp":1714628032002,"user_tz":-540,"elapsed":38425,"user":{"displayName":"유재하","userId":"18044117162550106446"}},"outputId":"eaed7c2f-3247-4131-eb14-e63bcaa92e6d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["# declare 4 bits quantize\n","quantization_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=torch.float16\n",")\n","# load 4 bits model\n","model = AutoModelForCausalLM.from_pretrained(SLLM_MODEL_ID,\n","                                             device_map='auto',\n","                                             quantization_config=quantization_config,\n","                                             token=HF_TOKEN)\n","# load tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(SLLM_MODEL_ID,\n","                                          add_special_tokens=True,\n","                                          token=HF_TOKEN)\n","tokenizer.padding_side = 'right'\n","\n","#pipe line\n","pipe = pipeline(\"text-generation\",\n","                model=model,\n","                tokenizer=tokenizer,\n","                max_new_tokens=512)\n","pipe"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":514},"id":"f06cAqchpUft","executionInfo":{"status":"error","timestamp":1714628283076,"user_tz":-540,"elapsed":825,"user":{"displayName":"유재하","userId":"18044117162550106446"}},"outputId":"b34a9a96-6dbb-45ee-ae97-5154d12160a9"},"execution_count":null,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"\n                    Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the\n                    quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules\n                    in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom `device_map` to\n                    `from_pretrained`. Check\n                    https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\n                    for more details.\n                    ","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-580b352b670d>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# load 4 bits model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m model = AutoModelForCausalLM.from_pretrained(SLLM_MODEL_ID,\n\u001b[0m\u001b[1;32m      9\u001b[0m                                              \u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                                              \u001b[0mquantization_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquantization_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    562\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3451\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhf_quantizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3452\u001b[0;31m                 \u001b[0mhf_quantizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_environment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3454\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mdevice_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/quantizers/quantizer_bnb_4bit.py\u001b[0m in \u001b[0;36mvalidate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m             }\n\u001b[1;32m     85\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdevice_map_without_lm_head\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"disk\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdevice_map_without_lm_head\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m     87\u001b[0m                     \"\"\"\n\u001b[1;32m     88\u001b[0m                     \u001b[0mSome\u001b[0m \u001b[0mmodules\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdispatched\u001b[0m \u001b[0mon\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mCPU\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdisk\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mMake\u001b[0m \u001b[0msure\u001b[0m \u001b[0myou\u001b[0m \u001b[0mhave\u001b[0m \u001b[0menough\u001b[0m \u001b[0mGPU\u001b[0m \u001b[0mRAM\u001b[0m \u001b[0mto\u001b[0m \u001b[0mfit\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: \n                    Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the\n                    quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules\n                    in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom `device_map` to\n                    `from_pretrained`. Check\n                    https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\n                    for more details.\n                    "]}]},{"cell_type":"code","source":["def rag_query(bm25, model, chunk_embeddings, query, tokens, chunk_list):\n","    result1 = query_bm25(bm25, query, tokens)\n","    # print(result1)\n","    info_list1 = [chunk_list[i] for i in result1]\n","\n","    result2 = query_sentence_transformer(model, chunk_embeddings, query)\n","    # print(result2)\n","    info_list2 = [chunk_list[i] for i in result2]\n","    messages = [\n","    {\n","        \"role\": \"user\",\n","        \"content\": f\"\"\"'문서1'부터 '문서15'를 참고하고 너의 지식을 이용하여 '질문'의 문맥에 맞게 답변하라.:\n","\n","\n","        문서15: {info_list2[9]}\n","\n","        문서14: {info_list2[8]}\n","\n","        문서13: {info_list2[7]}\n","\n","        문서12: {info_list2[6]}\n","\n","        문서11: {info_list2[5]}\n","\n","        문서10: {info_list1[4]}\n","\n","        문서9: {info_list1[3]}\n","\n","        문서8: {info_list1[2]}\n","\n","        문서7: {info_list1[1]}\n","\n","        문서6: {info_list1[0]}\n","\n","        문서5: {info_list2[4]}\n","\n","        문서4: {info_list2[3]}\n","\n","        문서3: {info_list2[2]}\n","\n","        문서2: {info_list2[1]}\n","\n","        문서1: {info_list2[0]}\n","\n","        질문: {query}\"\"\"\n","    }\n","    ]\n","    prompt = pipe.tokenizer.apply_chat_template(messages,\n","                                            tokenize=False,\n","                                            add_generation_prompt=True)\n","    # print(prompt)\n","\n","    outputs = pipe(\n","        prompt,\n","        do_sample=True,\n","        temperature=0.2,\n","        top_k=50,\n","        top_p=0.95,\n","        add_special_tokens=True\n","    )\n","    print('답변 \\n', outputs[0][\"generated_text\"][len(prompt):])"],"metadata":{"id":"Zse4KGWrLx_N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["while True:\n","    query = input('질문 : ')\n","    if len(query) == 0:\n","        break\n","    tokens = tokenizer1(query)\n","    rag_query(bm25, model_st, chunk_embeddings, query, tokens, chunk_list)\n","    print('-'*100)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KV5X6JqPv5p2","outputId":"607d975e-7302-4969-89e9-ada72bd8aabd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["질문 : 손흥민이 누구야?\n","답변 \n"," 문서에서 제공된 정보를 통해 손흥민의 이름은 언제나 나오지 않아 답변할 수 없습니다.\n","----------------------------------------------------------------------------------------------------\n","질문 : 저현고등학교가 뭐야?\n","답변 \n"," 문서3: 저현고등학교는 경기도 고양시 일산동구 식사동에 있는 공립 고등학교이다.\n","----------------------------------------------------------------------------------------------------\n","질문 : 아이폰의 역사\n","답변 \n"," **아이폰의 역사:**\n","\n","- 2007년: iPhone 2G- 1세대 아이폰이 출시, 멀티터치 기능과 2세대 통신망 기술 지원을 제공했다.\n","- 2008년: iPhone 3G- 2세대 아이폰이 출시, 3세대 통신망 기술 지원을 제공했다.\n","- 2009년: iPhone 3GS- 3세대 아이폰이 출시, 동영상 촬영이 가능하게 되었다.\n","- 2010년: iPhone 4- 4세대 아이폰이 출시, 디자인적 변화 요소가 많았다.\n","- 2011년: iPhone 4s- 5세대 아이폰이 출시, 스티브 잡스의 유작이라 불린다.\n","- 2012년: iPhone 5- 6세대 아이폰이 출시, 4인치 디스플레이와 4G 지원을 제공했다.\n","- 2014년: 아이폰 13이 출시, 6.1인치 디스플레이와 2015년 11월 11일 출시되었다.\n","- 2019년: 아이폰 11 프로가 출시, 아이폰 11과 같은 기능을 제공했다.\n","- 2021년: 아이폰 13 프로와 아이폰 13 프로 맥스가 출시, 16세대 플래그십 아이폰으로 출시되었다.\n","----------------------------------------------------------------------------------------------------\n","질문 : 삼성 갤럭시의 역사\n","답변 \n"," **질문: 삼성 갤럭시의 역사**\n","\n","문서를 통해 삼성전자의 보급형 스마트폰인 삼성 갤럭시의 역사를 확인할 수 있습니다.\n","\n","- **2017년 삼성 갤럭시 언팩 행사에서 공개:** 삼성 갤럭시 스마트워치가 처음 공개되었습니다.\n","\n","\n","- **2023년 12월 삼성 갤럭시 핏e 공개:** 삼성 갤럭시 A15가 출시되었습니다.\n","\n","\n","- **2021년 삼성 갤럭시 M52 5G 공개:** 삼성 갤럭시 M52 5G가 출시되었습니다.\n","\n","\n","- **2018년 삼성 갤럭시 와이드3 공개:** 삼성 갤럭시 와이드3가 출시되었습니다.\n","\n","\n","- **2015년 삼성 갤럭시 E 시리즈 출시:** 삼성 갤럭시 E 시리즈가 출시되었습니다.\n","\n","\n","- **2013년 삼성 갤럭시 J 시리즈 출시:** 삼성 갤럭시 J 시리즈가 출시되었습니다.\n","----------------------------------------------------------------------------------------------------\n","질문 : 갤럭시s의 역사\n","답변 \n"," **갤럭시s의 역사:**\n","\n","- 2010년 델 스트릭이 패블릿을 출시하여 휴대전화 시장에서 경쟁을 시작.\n","\n","\n","- 2011년 갤럭시 노트 5.3인치 화면을 사용하여 출시.\n","\n","\n","- 2014년 7월 1일, 바다, 윈도우 모바일에 대한 지원을 중단하고, 삼성 앱스는 갤럭시 앱스로 개편되었다.\n","\n","\n","- 2015년 7월 14일 갤럭시 s6 엣지(SM-G925S/K/L)의 업데이트가 실시되었다.\n","\n","\n","- 2016년 2월 15일 갤럭시 S6 엣지(SM-925S,K,L)의 업데이트가 실시되었다.\n","\n","\n","- 2017년 3월 11일 유럽버전의 갤럭시 S6/Edge의 7.0누가버전이 업데이트되었다.\n","\n","\n","- 2018년 3월부터 보안패치가 중단되었다.\n","----------------------------------------------------------------------------------------------------\n","질문 : 리그오브레전드\n","답변 \n"," 질문은 '레전드 오브 레전드'에 대한 질문입니다. 따라서 '레전드 오브 레전드'를 참고하여 답변을 드리도록 하겠습니다.\n","\n","'레전드 오브 레전드'는 2016년부터 2022년까지 방영된 미국의 드라마입니다.\n","----------------------------------------------------------------------------------------------------\n","질문 : 리그오브레전드 게임\n","답변 \n"," **리그오브레전드 게임에 대한 질문**\n","\n","1. 리그오브레전드는 무엇을 의미하는가요?\n","**답변:** 리그오브레전드는 라이엇게임즈가 주최하고 나이스게임TV가 주관하는 대회로, 온게임넷의 리그 오브 레전드 주관 대회의 하부리그로 진행됩니다.\n","\n","\n","2. 리그오브레전드는 어떤 게임을 의미하는가요?\n","**답변:** 리그오브레전드는 게임 리그 오브 레전드의 일종으로, 4인으로 조종하여 던전을 돌파하는 게임입니다.\n","\n","\n","3. 리그오브레전드는 어떤 기간 동안 진행되는가요?\n","**답변:** 리그 오브 레전드는 연간리그로 진행됩니다.\n","\n","\n","4. 리그오브레전드에서 우승하는 팀은 무엇을 받는가요?\n","**답변:** 리그 오브 레전드 챔피언스의 우승팀은 리그오브레전드 더 챔피언스의 차기 시드를 부여받습니다.\n","\n","\n","5. 리그오브레전드에서 탈락하는 팀은 무엇을 받는가요?\n","**답변:** 리그 오브 레전드 챔피언스에서 탈락하는 팀은 플래티넘 리그에 합류하고, 다이아 리그에 합류하는 경우가 있습니다.\n","----------------------------------------------------------------------------------------------------\n"]}]},{"cell_type":"markdown","source":["## 9. 향후 성능 향상을 위한 방법에 대한 제안\n","- 문서 분할을 기계적 분할이 아닌 GPT-4를 이용해서 의미 기반으로 분할하는 방법도 있습니다.\n","- 검색 성능 향상을 위해서는 DPR 계열의 기술을 사용하길 추천합니다.\n","- 검색 성능 향상을 위해서 BM25와 DPR을 적절한 비율로 반영하면 더 좋은 성능을 얻을 수 있다는 연구 결과가 있습니다.\n","- 많은 문서를 사용하기 위해서 벡터DB를 사용하는 것을 추천드립니다.\n","- RAG 성능을 높이는 방법으로는 추가적인 fine-tuining을 해 보는 것을 추천드립니다.\n","- RAG를 학습을 위한 데이터는 예상되는 질문 및 관련 문서 쌍을 1,000개 이상 만들고 GPT-4에게 질문을 해보고 그 답변을 정답으로 사용해서, SLLM을 fine-tuining 해 보는 것도 좋은 방법입니다."],"metadata":{"id":"ejVEj4gUTNqR"}},{"cell_type":"code","source":[],"metadata":{"id":"oG3Jt_WaURhU"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"collapsed_sections":["m0RiT4MhJc0W","iAgMMfZGVMJs","t6jHfNM5Vkbd","q8UFzERELe1f","8KHQwiSgOLiM","uHcQJeJf8UoQ","BgieRr5W8UoR","RNV_4xGy8lTG","UqzqXHO8JgQk","ejVEj4gUTNqR"],"gpuType":"T4","machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"81d32f1bf83c46b897382bdc6de7d6d6":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_591b6631bb384e11a35d85cce1cdd741","IPY_MODEL_a737b9b35b094ab390f514f3f711cf71","IPY_MODEL_1888212f177445d7b7c01d4dd99e8f3a","IPY_MODEL_1b5d4e719e2942ba99c177974b8d12af","IPY_MODEL_7ef1f9a8c4754e2e9020bf14b5523f26"],"layout":"IPY_MODEL_2738d31dfac947f89772d49dc7c15d3a"}},"591b6631bb384e11a35d85cce1cdd741":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_568b93f361584628bc8d404e380d89c5","placeholder":"​","style":"IPY_MODEL_39f2335b0e3f41a6a031c0990b1e0237","value":"<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"}},"a737b9b35b094ab390f514f3f711cf71":{"model_module":"@jupyter-widgets/controls","model_name":"PasswordModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"PasswordModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"PasswordView","continuous_update":true,"description":"Token:","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_7c4aa66d09a14780b2204caa82300b8c","placeholder":"​","style":"IPY_MODEL_fdf46c26dd9e4f41af5bab25476965e1","value":""}},"1888212f177445d7b7c01d4dd99e8f3a":{"model_module":"@jupyter-widgets/controls","model_name":"CheckboxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"CheckboxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"CheckboxView","description":"Add token as git credential?","description_tooltip":null,"disabled":false,"indent":true,"layout":"IPY_MODEL_52b54930303e4fab9c8dde1d2d4d7ed8","style":"IPY_MODEL_3198253cdb7149d48f7a702170a7b7e1","value":true}},"1b5d4e719e2942ba99c177974b8d12af":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"Login","disabled":false,"icon":"","layout":"IPY_MODEL_e0ae0196609b41a084438a0a70bf97bb","style":"IPY_MODEL_fcf9dc6d4cfd42d5bfdecc939b4cb63a","tooltip":""}},"7ef1f9a8c4754e2e9020bf14b5523f26":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b699c8057cbf4589a1f4ac90e81e0c85","placeholder":"​","style":"IPY_MODEL_1f2ed588dea34ac997a4f9af98c6825e","value":"\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"}},"2738d31dfac947f89772d49dc7c15d3a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":"center","align_self":null,"border":null,"bottom":null,"display":"flex","flex":null,"flex_flow":"column","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"50%"}},"568b93f361584628bc8d404e380d89c5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"39f2335b0e3f41a6a031c0990b1e0237":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7c4aa66d09a14780b2204caa82300b8c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fdf46c26dd9e4f41af5bab25476965e1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"52b54930303e4fab9c8dde1d2d4d7ed8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3198253cdb7149d48f7a702170a7b7e1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e0ae0196609b41a084438a0a70bf97bb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fcf9dc6d4cfd42d5bfdecc939b4cb63a":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"b699c8057cbf4589a1f4ac90e81e0c85":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1f2ed588dea34ac997a4f9af98c6825e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e7cb4b12d6f74077a3b37b12b5f9c6d2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_30a1122259ad45c4b727e52a8fbfe1d9","IPY_MODEL_baf69db8f2cc4aefa82cdf842f73318c","IPY_MODEL_cfba60f9cf9c4cc982272572a3e61433"],"layout":"IPY_MODEL_7ccf8add4c3d4672a0e1061fe23758cf"}},"30a1122259ad45c4b727e52a8fbfe1d9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7636c589cc9b4c3eb1fce18bc3cd3534","placeholder":"​","style":"IPY_MODEL_234893dfd5da4db989136ebc51f4f1cb","value":"Loading checkpoint shards: 100%"}},"baf69db8f2cc4aefa82cdf842f73318c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d71cee5477634239ba776cbf9c42eb27","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6578ccfa5a1f44a09ea3924d2ab10b79","value":2}},"cfba60f9cf9c4cc982272572a3e61433":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c03b94eb056d420c88fff40c5c171976","placeholder":"​","style":"IPY_MODEL_296753054e8d4c6a82f8d6150e3ebd11","value":" 2/2 [00:10&lt;00:00,  4.81s/it]"}},"7ccf8add4c3d4672a0e1061fe23758cf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7636c589cc9b4c3eb1fce18bc3cd3534":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"234893dfd5da4db989136ebc51f4f1cb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d71cee5477634239ba776cbf9c42eb27":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6578ccfa5a1f44a09ea3924d2ab10b79":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c03b94eb056d420c88fff40c5c171976":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"296753054e8d4c6a82f8d6150e3ebd11":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}